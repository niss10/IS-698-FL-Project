{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fSIlfbJYkb1S",
        "_WNZB-aUSq6e",
        "zGRnhpzxUUnQ",
        "yJXrdJyV7Oec",
        "_YQaJWQg7YUU",
        "-udMc-P97cAO",
        "CH-BSUy-7y-V",
        "DaDjUBlhV9Y1",
        "jDw9nfRjAKSX"
      ],
      "authorship_tag": "ABX9TyN7VRHwBu1gaA1Ap0kIn4fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niss10/IS-698-FL-Project/blob/main/FedCollab_FL_latest_flower_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latest Flower implementation\n",
        "This notebook uses latest flower code. So it isa based on latest flower implementation"
      ],
      "metadata": {
        "id": "smw6nDYh5EZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite - Environment Setup"
      ],
      "metadata": {
        "id": "r_EuGWsABu8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Run the below cell\n",
        "2. Then, RESTART the Colab runtime. (after succefull completion)\n",
        "3. After Restarting don't run this cell"
      ],
      "metadata": {
        "id": "bXlthzgsiNS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Force‑reinstall NumPy and scikit‑learn\n",
        "!pip install --upgrade --force-reinstall numpy==1.24.3 scikit-learn\n",
        "\n",
        "# Install the CPU‑only PyTorch build\n",
        "!pip uninstall -y torch torchvision\n",
        "!pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision\n",
        "\n",
        "# Install/latest versions of Flower and PyTorch\n",
        "!pip install --upgrade flwr pandas\n",
        "!pip install -U \"flwr[simulation]\"\n",
        "\n",
        "# Then, RESTART the Colab runtime.\n",
        "\n",
        "# After Restarting don't run this cell"
      ],
      "metadata": {
        "id": "-EPVaSB5BvjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab36e59a-d92b-4c56-aba3-ec83185ef09d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Using cached scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Using cached scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flwr 1.18.0 requires numpy<3.0.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cpu which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.0 numpy-1.24.3 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2af208c04cb5479a9a118aec79690056"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.7.0+cpu\n",
            "Uninstalling torch-2.7.0+cpu:\n",
            "  Successfully uninstalled torch-2.7.0+cpu\n",
            "Found existing installation: torchvision 0.22.0+cpu\n",
            "Uninstalling torchvision-0.22.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.22.0+cpu\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (176.0 MB)\n",
            "Using cached https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: torch, torchvision\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cpu which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.7.0+cpu torchvision-0.22.0+cpu\n",
            "Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n",
            "Collecting numpy<3.0.0,>=1.26.0 (from flwr)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.7)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.13.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cpu which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.5)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.7)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.13.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "TBZ5pIsvpuQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import flwr as fl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from flwr.server.strategy import FedAvg\n",
        "import time\n",
        "from flwr.common import Metrics\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
        "from flwr.simulation import run_simulation\n",
        "from flwr.common import Context"
      ],
      "metadata": {
        "id": "aE2fGvEKn5Vs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Master file\n",
        "Setting source of truth file that we will use for federated learning simulation"
      ],
      "metadata": {
        "id": "1oWfHU7v6pti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the downloaded preprocessed_dataset.csv from Dataset/Pre-Processed/ folder"
      ],
      "metadata": {
        "id": "nDKbo1Hcm9C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Master dataset file\n",
        "print(\"Please upload preprocessed_dataset.csv\")\n",
        "uploaded = files.upload()  # upload the downloaded preprocessed_dataset.csv\n",
        "if(uploaded):\n",
        "  print(\"File uploaded successfully\")\n",
        "else:\n",
        "  print(\"File upload failed, Please upload again\")\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "aKWhgYvU6o5c",
        "outputId": "ab3f3810-64cf-4684-efc4-3644ba153802"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload preprocessed_dataset.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c780f3fa-4771-45d4-8cca-254ed22a6309\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c780f3fa-4771-45d4-8cca-254ed22a6309\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preprocessed_dataset.csv to preprocessed_dataset.csv\n",
            "File uploaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visulation Utilities"
      ],
      "metadata": {
        "id": "fSIlfbJYkb1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph Plot\n",
        "def plot_metrics(mse_values, rmse_values, num_clients, num_rounds, num_epochs):\n",
        "    mse_values = mse_values\n",
        "    rmse_values = rmse_values\n",
        "    rounds = range(1, num_rounds + 1)\n",
        "    output_path = f\"metrics_plot_{num_clients}clients_{num_rounds}rounds_{num_epochs}epochs.png\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(rounds, mse_values, marker=\"o\", label=\"MSE\")\n",
        "    plt.plot(rounds, rmse_values, marker=\"s\", label=\"RMSE\")\n",
        "    plt.xlabel(\"Round\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(f\"Federated Learning MSE and RMSE ({num_clients} Clients, {num_rounds} Rounds), {num_epochs} epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"Metrics plot saved to {output_path}\")\n",
        "    return mse_values, rmse_values"
      ],
      "metadata": {
        "id": "ExJhAZvUkoF8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Partitioning for Client simulations"
      ],
      "metadata": {
        "id": "_WNZB-aUSq6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P-lQGAfFQrWN"
      },
      "outputs": [],
      "source": [
        "# Data Partitioning Utility\n",
        "\n",
        "# Load the master preprocessed dataset.\n",
        "def load_master(csv_path):\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "# Randomly select N different user from all\n",
        "def sample_user_ids(df, num_clients, seed=42):\n",
        "    user_ids = df[\"UserID\"].unique()\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if num_clients >= len(user_ids):\n",
        "        return list(user_ids)\n",
        "    return list(rng.choice(user_ids, size=num_clients, replace=False))\n",
        "\n",
        "# Save each selected user's rating to out_dir/user_{uid}.csv\n",
        "def partition_by_user(df, out_dir, user_ids=None):\n",
        "    out_path = Path(out_dir)\n",
        "    out_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if user_ids is None:\n",
        "        user_ids = df[\"UserID\"].unique()\n",
        "\n",
        "    csv_paths = []\n",
        "    for uid in user_ids:\n",
        "        df_user = df[df[\"UserID\"] == uid]\n",
        "        path = out_path / f\"user_{uid}.csv\"\n",
        "        df_user.to_csv(path, index=False)\n",
        "        csv_paths.append(str(path))\n",
        "    return csv_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check data of partioning"
      ],
      "metadata": {
        "id": "_NZBR6sd887m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the master dataset that we have uploaded earlier\n",
        "MASTER_CSV = \"preprocessed_dataset.csv\"\n",
        "df_master  = load_master(MASTER_CSV)\n",
        "print(\"Rows:\", len(df_master), \" | Users:\", df_master['UserID'].nunique(), \" | Movie:\", df_master['MovieID'].nunique())\n",
        "\n",
        "# Cheking for 5 clients\n",
        "paths_5   = partition_by_user(df_master, \"clients_5\",   sample_user_ids(df_master, 5))\n",
        "\n",
        "\n",
        "print(f\"5-client num files:   {len(paths_5)}\")\n",
        "print(f\"5-client files:   {paths_5}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma7DZJW0SYQQ",
        "outputId": "9604e237-45f0-4d49-bd17-c11186ac57b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2292d8e7e956>:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(csv_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1000209  | Users: 6040  | Movie: 3706\n",
            "5-client num files:   5\n",
            "5-client files:   ['clients_5/user_4673.csv', 'clients_5/user_2651.csv', 'clients_5/user_3953.csv', 'clients_5/user_539.csv', 'clients_5/user_2616.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Local-Training Utilities\n",
        "In this section we are difining model and how model will train data we have used REcommenderMLP using Pytorch for training."
      ],
      "metadata": {
        "id": "zGRnhpzxUUnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Local data and Train/Test Split"
      ],
      "metadata": {
        "id": "yJXrdJyV7Oec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return train & test DataLoaders for one client CSV.\n",
        "def load_client_data(csv_path, batch_size=32, test_size=0.2, seed=42):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    users  = torch.tensor(df[\"UserID\"].values,   dtype=torch.long)\n",
        "    movies = torch.tensor(df[\"MovieID\"].values,  dtype=torch.long)\n",
        "    ratings= torch.tensor(df[\"Rating\"].values,   dtype=torch.float32)\n",
        "\n",
        "    idx = np.arange(len(df))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(idx)\n",
        "    split = int((1 - test_size) * len(idx))\n",
        "    train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    train_ds = TensorDataset(users[train_idx], movies[train_idx], ratings[train_idx])\n",
        "    test_ds  = TensorDataset(users[test_idx],  movies[test_idx],  ratings[test_idx])\n",
        "\n",
        "    return (DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
        "            DataLoader(test_ds,  batch_size=batch_size, shuffle=False))"
      ],
      "metadata": {
        "id": "khTswnzQR52R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "_YQaJWQg7YUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining RecommenderMLP Model using Pytorch\n",
        "\n",
        "# User-embedding + Movie-embedding -> 2-layer MLP[64 -> ReLu] -> rating (1-5).\n",
        "class RecommenderMLP(nn.Module):\n",
        "    def __init__(self, num_users, num_movies,\n",
        "                 embed_dim=32, hidden=64):\n",
        "        super().__init__()\n",
        "        self.user_embed  = nn.Embedding(num_users  + 1, embed_dim)\n",
        "        self.movie_embed = nn.Embedding(num_movies + 1, embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        u = self.user_embed(user_ids)\n",
        "        m = self.movie_embed(movie_ids)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze(1)"
      ],
      "metadata": {
        "id": "REPPa2DoUWvv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "-udMc-P97cAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility for loading data into tensor for client and training and evalation\n",
        "\n",
        "# this function will train Model for one epoch\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total, count = 0.0, 0\n",
        "    for u, m, r in loader:\n",
        "        u, m, r = u.to(device), m.to(device), r.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(u, m), r)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item() * len(r)\n",
        "        count += len(r)\n",
        "    return total / count\n",
        "\n",
        "# Evaluate train model on test dataset and return loss\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total, count = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for u, m, r in loader:\n",
        "            u, m, r = u.to(device), m.to(device), r.to(device)\n",
        "            loss = criterion(model(u, m), r)\n",
        "            total += loss.item() * len(r)\n",
        "            count += len(r)\n",
        "    return total / count"
      ],
      "metadata": {
        "id": "ebMR_JL1oG2o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Sanity Check of Model"
      ],
      "metadata": {
        "id": "aLh8_q7wV1Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# running model for 1 user from 5 client we have created to check how it works\n",
        "sample_csv = \"clients_5/\" + sorted(os.listdir(\"clients_5\"))[0]\n",
        "\n",
        "# Get dataset dimensions for embeddings\n",
        "num_users  = df_master[\"UserID\"].max()\n",
        "num_movies = df_master[\"MovieID\"].max()\n",
        "\n",
        "# Build loaders and model\n",
        "train_loader, test_loader = load_client_data(sample_csv, batch_size=16)\n",
        "device = torch.device(\"cpu\")\n",
        "model  = RecommenderMLP(num_users, num_movies).to(device)\n",
        "\n",
        "opt  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit = torch.nn.MSELoss()\n",
        "\n",
        "Train_loss = train_one_epoch(model, train_loader, opt, crit, device)\n",
        "Test_loss  = evaluate(model, test_loader, crit, device)\n",
        "\n",
        "print(\"Train loss:\", Train_loss)\n",
        "print(\"Test  loss:\", Test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hvfbRxdV3SI",
        "outputId": "38fc0a64-ecf4-4c1c-afc7-6dc3fd85d47d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 12.165363311767578\n",
            "Test  loss: 13.152181625366211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FL Setup"
      ],
      "metadata": {
        "id": "CH-BSUy-7y-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL Client"
      ],
      "metadata": {
        "id": "QoFAfpYm8hNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower Client\n",
        "class FLClient(NumPyClient):\n",
        "    def __init__(self, model, train_loader, test_loader, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return [p.cpu().numpy() for p in self.model.state_dict().values()]\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        keys = list(self.model.state_dict().keys())\n",
        "        self.model.load_state_dict({k: torch.tensor(v) for k, v in zip(keys, params)}, strict=True)\n",
        "\n",
        "    def fit(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        num_epochs = CONFIG[\"num_epochs\"]\n",
        "        for _ in range(num_epochs):\n",
        "           local_train_loss = train_one_epoch(self.model, self.train_loader, self.optimizer, self.criterion, self.device)\n",
        "           print(\"Local training loss:\", local_train_loss)\n",
        "        return self.get_parameters(config), len(self.train_loader.dataset), {}\n",
        "\n",
        "    def evaluate(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        loss = evaluate(self.model, self.test_loader, self.criterion, self.device)\n",
        "        return float(loss), len(self.test_loader.dataset), {}\n",
        "\n",
        "# Client Function or Client App\n",
        "def client_fn(context: Context) -> Client:\n",
        "    try:\n",
        "      # # Map node_id to a valid index (0 to num_clients-1)\n",
        "      # node_id = str(context.node_id)\n",
        "      idx = int(context.node_config[\"partition-id\"])\n",
        "      print(f\"Creating client {idx}\")\n",
        "      csv  = csv_paths[idx] # csv file per user that we have created earlier\n",
        "      client_training_data, client_test_data = load_client_data(csv, batch_size=32) # Creating training data and test data from those per user/clent csv file\n",
        "      model  = RecommenderMLP(num_users, num_movies, CONFIG[\"embed_dim\"], CONFIG[\"hidden_dim\"])\n",
        "      return FLClient(model, client_training_data, client_test_data, device=torch.device(CONFIG[\"device\"])).to_client()\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in client_fn: {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in client_fn for node_id {context.node_id}: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "C8B6PA0-8gOJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL Server\n"
      ],
      "metadata": {
        "id": "DaDjUBlhV9Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Server Function or Server App which has strategy how to FL will run\n",
        "def server_fn(context: Context) -> ServerAppComponents:\n",
        "    strategy = FedAvg(\n",
        "        fraction_fit=1,\n",
        "        fraction_evaluate=1,\n",
        "        min_fit_clients=CONFIG[\"num_clients\"],\n",
        "        min_evaluate_clients=CONFIG[\"num_clients\"],\n",
        "        min_available_clients=CONFIG[\"num_clients\"],\n",
        "    )\n",
        "    config = ServerConfig(num_rounds=CONFIG[\"num_rounds\"])\n",
        "    return ServerAppComponents(strategy=strategy, config=config)"
      ],
      "metadata": {
        "id": "jCgdF1hoWGbH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Runner"
      ],
      "metadata": {
        "id": "C0f4GhUu_79o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_federated():\n",
        "  global csv_paths, num_users, num_movies # we need these in Client Function\n",
        "  df_master = load_master(CONFIG[\"data_path\"])\n",
        "  selected_uids = sample_user_ids(df_master, CONFIG[\"num_clients\"])\n",
        "  folder = f\"clients_{CONFIG['num_clients']}\"\n",
        "  csv_paths = partition_by_user(df_master, folder, selected_uids)\n",
        "\n",
        "  # Set dataset dimensions\n",
        "  num_users = int(df_master[\"UserID\"].max())\n",
        "  num_movies = int(df_master[\"MovieID\"].max())\n",
        "\n",
        "  # Defining client and server app\n",
        "  client_app = ClientApp(client_fn=client_fn)\n",
        "  server_app = ServerApp(server_fn=server_fn)\n",
        "\n",
        "  # Run simulation\n",
        "  start_time = time.perf_counter()\n",
        "  run_simulation(\n",
        "            client_app=client_app,\n",
        "            server_app=server_app,\n",
        "            num_supernodes=CONFIG[\"num_clients\"],\n",
        "            backend_config={\n",
        "                \"client_resources\": {\n",
        "                    \"num_cpus\": 1,\n",
        "                }\n",
        "            },\n",
        "        )\n",
        "  end_time = time.perf_counter()\n",
        "  print(f\"Time taken: {end_time - start_time:0.4f} seconds\")\n",
        "  # for r, mse in history.losses_distributed:\n",
        "  #       print(f\"Round {r:02d}: MSE={mse:.4f}, RMSE={np.sqrt(mse):.4f}\")\n",
        "  #       mse_values.append(mse)\n",
        "  #       rmse_values.append(np.sqrt(mse))\n",
        "\n",
        "  # return history, mse_values, rmse_values"
      ],
      "metadata": {
        "id": "R9fb9aBHhMrj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triggering Federated Learning"
      ],
      "metadata": {
        "id": "jDw9nfRjAKSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"num_clients\": 100,\n",
        "    \"num_rounds\": 10,\n",
        "    \"num_epochs\": 3,\n",
        "    \"data_path\": \"preprocessed_dataset.csv\",\n",
        "    \"device\": \"cpu\",\n",
        "    \"embed_dim\": 32,\n",
        "    \"hidden_dim\": 64,\n",
        "}\n",
        "\n",
        "run_federated()\n",
        "# history, mse_values, rmse_values   = run_federated()\n",
        "# plot_metrics( mse_values, rmse_values, CONFIG[\"num_clients\"], CONFIG[\"num_rounds\"], CONFIG[\"num_epochs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zOvgWz3XaNX",
        "outputId": "1b0b5103-a83f-473d-f706-bfd787169a74"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2292d8e7e956>:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(csv_path)\n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=10, no round_timeout\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=23110)\u001b[0m 2025-05-15 21:36:44.443875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=23110)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=23110)\u001b[0m E0000 00:00:1747345004.487105   23110 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=23110)\u001b[0m E0000 00:00:1747345004.502240   23110 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 32\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.085717480592054\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 4.207118089752968\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.6949916439827042\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 34\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 17.36362490446671\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.979837334674338\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.689846536387568\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 40\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.10493431510506\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.100400159647176\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 6.543750066023606\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 81\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 18.49304823441939\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 17.17072972384366\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.89013957977295\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 92\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.599108986232592\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 7.766328998233961\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 5.360078162041264\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 4\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 16.427629470825195\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.662652015686035\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.908767700195312\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 91\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.985261721783374\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.318767771663437\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 5.303045134946524\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 29\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.19747768960348\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.790725056718035\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 12.483685330646795\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 31\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.556454765662718\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.411894753631135\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 4.132240272639843\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 5\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.504398345947266\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 5.02083362612808\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 2.4648256009085134\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 12\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.76768447028266\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.682451022112812\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 7.868729446552418\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 88\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.592391478048789\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 9.38918365014566\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.278647654765361\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 13\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 12.699583530426025\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.578600406646729\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.490994930267334\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 0\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.412351608276367\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.782434463500977\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.172149658203125\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 21\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 18.77268746319939\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 17.265449149935854\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.783961763568954\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 1\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.892554348912732\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.624255311900171\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.414061644981647\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 8\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 23.305093220302037\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 21.745949063982284\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 20.29191474914551\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 45\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 17.920133198008816\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.901662826538086\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.073820114135742\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 38\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.470281067964072\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 3.106889761051285\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.8705752907512343\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 10.234225818089076\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 9.163106645856585\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 8.132505280630928\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 18.930408477783203\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 17.326076067410984\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.778198388906626\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 72\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.59304903631341\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 12.963387005949674\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.441234680071268\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 9.751733054285465\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 7.102205587470013\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 4.706239264944325\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 16.012374531139027\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.529980399391867\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.125883622602982\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 16.505565643310547\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.63994312286377\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.79996109008789\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.965242385864258\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.322086334228516\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.69413948059082\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 9.213046202583918\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 3.5000176051306346\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.5623463061120775\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 15.857377370198568\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 14.414998372395834\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.066826820373535\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 16.121351623535155\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 13.7722318649292\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.587302589416504\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 11.878452773233063\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 9.295225356389018\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 6.922925222267225\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 15.375290552775065\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 80\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 11.738752365112305\u001b[32m [repeated 82x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 96\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 10.048245883224034\u001b[32m [repeated 77x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 17\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 19.4982852935791\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 45\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 12.455085496644717\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 17\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 10.986681548916565\u001b[32m [repeated 75x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m 20.17976897103446\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 7\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 15.770622216018976\u001b[32m [repeated 103x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 99\u001b[32m [repeated 37x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 3.0025955682777496\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 30\u001b[32m [repeated 57x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 6.436254514406805\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 31\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 6.28703234745906\u001b[32m [repeated 103x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m 3.6330876326320145\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 94\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.6213652377571564\u001b[32m [repeated 69x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 79\u001b[32m [repeated 23x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 3.0359723017765927\u001b[32m [repeated 113x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 11\u001b[32m [repeated 66x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 7.1164247512817385\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 96\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 5.895447572072347\u001b[32m [repeated 77x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 93\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 5.585117449525927\u001b[32m [repeated 116x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 1\u001b[32m [repeated 38x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.613850005741777\u001b[32m [repeated 101x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 48\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.4862089492656567\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 16\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 3.7157141933702444\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 52\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 3.2973744869232178\u001b[32m [repeated 111x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m  0.9250380737440926\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 66\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 3.5938792889661126\u001b[32m [repeated 66x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 36\u001b[32m [repeated 29x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.70765132129572\u001b[32m [repeated 60x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 1\u001b[32m [repeated 102x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 2.1945956715366295\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 79\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 6.704008102416992\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 18\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.4564993633542742\u001b[32m [repeated 118x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 19\u001b[32m [repeated 39x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 0.5694800042411656\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 57\u001b[32m [repeated 77x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.7713749920880353\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 67\u001b[32m [repeated 53x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.5182406378955375\u001b[32m [repeated 120x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 14\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 0.6725024054015892\u001b[32m [repeated 111x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 5\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.4417579460665177\u001b[32m [repeated 60x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 91\u001b[32m [repeated 73x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.538383637017351\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 24\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 0.6718740251617156\u001b[32m [repeated 107x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 25\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.9963558943648088\u001b[32m [repeated 84x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 60\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.9269818879188375\u001b[32m [repeated 79x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 38\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 1.259744766212645\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 36\u001b[32m [repeated 42x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 3.32425057888031\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 59\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.9410963048454092\u001b[32m [repeated 115x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 5\u001b[32m [repeated 37x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 2.1124685073302962\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 67\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 2.8261001110076904\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 78\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 1.405842433210279\u001b[32m [repeated 53x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 70\u001b[32m [repeated 92x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.5942742660425712\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 91\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Local training loss: 2.1187191631482993\u001b[32m [repeated 71x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 27\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.3805141065801893\u001b[32m [repeated 98x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 71\u001b[32m [repeated 36x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Local training loss: 0.9828659187663685\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=23110)\u001b[0m Creating client 19\u001b[32m [repeated 59x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 10 round(s) in 228.83s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 10.040808862080848\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 7.707551913352836\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 5.931757429593988\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 4.638478415043635\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 3.710579690430407\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 6: 3.0540004573634607\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 7: 2.591321015115884\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 8: 2.277698053827079\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 9: 2.058650801960082\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 10: 1.907473060808978\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=23111)\u001b[0m Creating client 46\u001b[32m [repeated 45x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=23111)\u001b[0m 2025-05-15 21:36:44.460189: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=23111)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=23111)\u001b[0m E0000 00:00:1747345004.506189   23111 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=23111)\u001b[0m E0000 00:00:1747345004.519032   23111 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 254.2186 seconds\n"
          ]
        }
      ]
    }
  ]
}