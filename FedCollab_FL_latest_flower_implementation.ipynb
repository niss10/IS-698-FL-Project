{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM422LPn450KMrTNDhPWX3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niss10/IS-698-FL-Project/blob/main/FedCollab_FL_latest_flower_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latest Flower implementation\n",
        "This notebook uses latest flower code. So it is based on latest flower implementation"
      ],
      "metadata": {
        "id": "smw6nDYh5EZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite - Environment Setup"
      ],
      "metadata": {
        "id": "r_EuGWsABu8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Run the below cell\n",
        "2. Then, RESTART the Colab runtime. (after succefull completion)\n",
        "3. After Restarting don't run this cell"
      ],
      "metadata": {
        "id": "bXlthzgsiNS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Force‑reinstall NumPy and scikit‑learn\n",
        "!pip install --upgrade --force-reinstall numpy==1.24.3 scikit-learn\n",
        "\n",
        "# Install the CPU‑only PyTorch build\n",
        "!pip uninstall -y torch torchvision\n",
        "!pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision\n",
        "\n",
        "# Install/latest versions of Flower and PyTorch\n",
        "!pip install --upgrade flwr pandas\n",
        "!pip install -U \"flwr[simulation]\"\n",
        "\n",
        "# Then, RESTART the Colab runtime.\n",
        "\n",
        "# After Restarting don't run this cell"
      ],
      "metadata": {
        "id": "-EPVaSB5BvjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a8cb090-5250-4bea-d922-cf36a2204cfd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.0 numpy-1.24.3 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "1d15f2ea7c0444dca17d775f84cbbf83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (176.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.22.0%2Bcpu-cp311-cp311-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy, torch, torchvision\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cpu which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3 torch-2.7.0+cpu torchvision-0.22.0+cpu\n",
            "Collecting flwr\n",
            "  Downloading flwr-1.18.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cryptography<45.0.0,>=44.0.1 (from flwr)\n",
            "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.71.0)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr)\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting numpy<3.0.0,>=1.26.0 (from flwr)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathspec<0.13.0,>=0.12.1 (from flwr)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.6 (from flwr)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (4.13.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
            "Downloading flwr-1.18.0-py3-none-any.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.0/540.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tomli-w, tomli, pycryptodome, protobuf, pathspec, numpy, iterators, pandas, cryptography, typer, flwr\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.3\n",
            "    Uninstalling typer-0.15.3:\n",
            "      Successfully uninstalled typer-0.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cpu which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cryptography-44.0.3 flwr-1.18.0 iterators-0.0.2 numpy-2.2.5 pandas-2.2.3 pathspec-0.12.1 protobuf-4.25.7 pycryptodome-3.22.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.5)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.7)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Collecting ray==2.31.0 (from flwr[simulation])\n",
            "  Downloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.13.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.24.0)\n",
            "Downloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl (66.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "TBZ5pIsvpuQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import flwr as fl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from flwr.server.strategy import FedAvg\n",
        "import time\n",
        "from flwr.common import Metrics\n",
        "from flwr.client import Client, ClientApp, NumPyClient\n",
        "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
        "from flwr.simulation import run_simulation\n",
        "from flwr.common import Context"
      ],
      "metadata": {
        "id": "aE2fGvEKn5Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Master file\n",
        "Setting source of truth file that we will use for federated learning simulation"
      ],
      "metadata": {
        "id": "1oWfHU7v6pti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the downloaded preprocessed_dataset.csv from Dataset/Pre-Processed/ folder"
      ],
      "metadata": {
        "id": "nDKbo1Hcm9C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Master dataset file\n",
        "print(\"Please upload preprocessed_dataset.csv\")\n",
        "uploaded = files.upload()  # upload the downloaded preprocessed_dataset.csv\n",
        "if(uploaded):\n",
        "  print(\"File uploaded successfully\")\n",
        "else:\n",
        "  print(\"File upload failed, Please upload again\")\n",
        "  uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "aKWhgYvU6o5c",
        "outputId": "5fb0cc1b-de24-4f35-d99b-7dfd004b849a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload preprocessed_dataset.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d417bf23-290d-4a76-bfb9-8d72da1dbdcc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d417bf23-290d-4a76-bfb9-8d72da1dbdcc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving preprocessed_dataset.csv to preprocessed_dataset.csv\n",
            "File uploaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Partitioning for Client simulations"
      ],
      "metadata": {
        "id": "_WNZB-aUSq6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-lQGAfFQrWN"
      },
      "outputs": [],
      "source": [
        "# Data Partitioning Utility\n",
        "\n",
        "# Load the master preprocessed dataset.\n",
        "def load_master(csv_path):\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "# Randomly select N different user from all\n",
        "def sample_user_ids(df, num_clients, seed=42):\n",
        "    user_ids = df[\"UserID\"].unique()\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if num_clients >= len(user_ids):\n",
        "        return list(user_ids)\n",
        "    return list(rng.choice(user_ids, size=num_clients, replace=False))\n",
        "\n",
        "# Save each selected user's rating to out_dir/user_{uid}.csv\n",
        "def partition_by_user(df, out_dir, user_ids=None):\n",
        "    out_path = Path(out_dir)\n",
        "    out_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if user_ids is None:\n",
        "        user_ids = df[\"UserID\"].unique()\n",
        "\n",
        "    csv_paths = []\n",
        "    for uid in user_ids:\n",
        "        df_user = df[df[\"UserID\"] == uid]\n",
        "        path = out_path / f\"user_{uid}.csv\"\n",
        "        df_user.to_csv(path, index=False)\n",
        "        csv_paths.append(str(path))\n",
        "    return csv_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check data of partioning"
      ],
      "metadata": {
        "id": "_NZBR6sd887m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the master dataset that we have uploaded earlier\n",
        "MASTER_CSV = \"preprocessed_dataset.csv\"\n",
        "df_master  = load_master(MASTER_CSV)\n",
        "print(\"Rows:\", len(df_master), \" | Users:\", df_master['UserID'].nunique(), \" | Movie:\", df_master['MovieID'].nunique())\n",
        "\n",
        "# Cheking for 5 clients\n",
        "paths_5   = partition_by_user(df_master, \"clients_5\",   sample_user_ids(df_master, 5))\n",
        "\n",
        "\n",
        "print(f\"5-client num files:   {len(paths_5)}\")\n",
        "print(f\"5-client files:   {paths_5}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma7DZJW0SYQQ",
        "outputId": "26894a14-ed33-44cc-f3ec-7d72fd869e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2292d8e7e956>:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(csv_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1000209  | Users: 6040  | Movie: 3706\n",
            "5-client num files:   5\n",
            "5-client files:   ['clients_5/user_4673.csv', 'clients_5/user_2651.csv', 'clients_5/user_3953.csv', 'clients_5/user_539.csv', 'clients_5/user_2616.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Local-Training Utilities\n",
        "In this section we are difining model and how model will train data we have used REcommenderMLP using Pytorch for training."
      ],
      "metadata": {
        "id": "zGRnhpzxUUnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Local data and Train/Test Split"
      ],
      "metadata": {
        "id": "yJXrdJyV7Oec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return train & test DataLoaders for one client CSV.\n",
        "def load_client_data(csv_path, batch_size=32, test_size=0.2, seed=42):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    users  = torch.tensor(df[\"UserID\"].values,   dtype=torch.long)\n",
        "    movies = torch.tensor(df[\"MovieID\"].values,  dtype=torch.long)\n",
        "    ratings= torch.tensor(df[\"Rating\"].values,   dtype=torch.float32)\n",
        "\n",
        "    idx = np.arange(len(df))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(idx)\n",
        "    split = int((1 - test_size) * len(idx))\n",
        "    train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    train_ds = TensorDataset(users[train_idx], movies[train_idx], ratings[train_idx])\n",
        "    test_ds  = TensorDataset(users[test_idx],  movies[test_idx],  ratings[test_idx])\n",
        "\n",
        "    return (DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
        "            DataLoader(test_ds,  batch_size=batch_size, shuffle=False))"
      ],
      "metadata": {
        "id": "khTswnzQR52R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "_YQaJWQg7YUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining RecommenderMLP Model using Pytorch\n",
        "\n",
        "# User-embedding + Movie-embedding -> 2-layer MLP[64 -> ReLu] -> rating (1-5).\n",
        "class RecommenderMLP(nn.Module):\n",
        "    def __init__(self, num_users, num_movies,\n",
        "                 embed_dim=32, hidden=64):\n",
        "        super().__init__()\n",
        "        self.user_embed  = nn.Embedding(num_users  + 1, embed_dim)\n",
        "        self.movie_embed = nn.Embedding(num_movies + 1, embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids, movie_ids):\n",
        "        u = self.user_embed(user_ids)\n",
        "        m = self.movie_embed(movie_ids)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze(1)"
      ],
      "metadata": {
        "id": "REPPa2DoUWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "-udMc-P97cAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility for loading data into tensor for client and training and evalation\n",
        "\n",
        "# this function will train Model for one epoch\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total, count = 0.0, 0\n",
        "    for u, m, r in loader:\n",
        "        u, m, r = u.to(device), m.to(device), r.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(u, m), r)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item() * len(r)\n",
        "        count += len(r)\n",
        "    return total / count\n",
        "\n",
        "# Evaluate train model on test dataset and return loss\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total, count = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for u, m, r in loader:\n",
        "            u, m, r = u.to(device), m.to(device), r.to(device)\n",
        "            loss = criterion(model(u, m), r)\n",
        "            total += loss.item() * len(r)\n",
        "            count += len(r)\n",
        "    return total / count"
      ],
      "metadata": {
        "id": "ebMR_JL1oG2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Sanity Check of Model"
      ],
      "metadata": {
        "id": "aLh8_q7wV1Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# running model for 1 user from 5 client we have created to check how it works\n",
        "sample_csv = \"clients_5/\" + sorted(os.listdir(\"clients_5\"))[0]\n",
        "\n",
        "# Get dataset dimensions for embeddings\n",
        "num_users  = df_master[\"UserID\"].max()\n",
        "num_movies = df_master[\"MovieID\"].max()\n",
        "\n",
        "# Build loaders and model\n",
        "train_loader, test_loader = load_client_data(sample_csv, batch_size=16)\n",
        "device = torch.device(\"cpu\")\n",
        "model  = RecommenderMLP(num_users, num_movies).to(device)\n",
        "\n",
        "opt  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit = torch.nn.MSELoss()\n",
        "\n",
        "Train_loss = train_one_epoch(model, train_loader, opt, crit, device)\n",
        "Test_loss  = evaluate(model, test_loader, crit, device)\n",
        "\n",
        "print(\"Train loss:\", Train_loss)\n",
        "print(\"Test  loss:\", Test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hvfbRxdV3SI",
        "outputId": "77db1a98-7698-4c2b-f893-08b1088057f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 12.05018367767334\n",
            "Test  loss: 12.613490104675293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FL Setup"
      ],
      "metadata": {
        "id": "CH-BSUy-7y-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL Client"
      ],
      "metadata": {
        "id": "QoFAfpYm8hNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower Client\n",
        "class FLClient(NumPyClient):\n",
        "    def __init__(self, model, train_loader, test_loader, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return [p.cpu().numpy() for p in self.model.state_dict().values()]\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        keys = list(self.model.state_dict().keys())\n",
        "        self.model.load_state_dict({k: torch.tensor(v) for k, v in zip(keys, params)}, strict=True)\n",
        "\n",
        "    def fit(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        num_epochs = CONFIG[\"num_epochs\"]\n",
        "        for _ in range(num_epochs):\n",
        "           local_train_loss = train_one_epoch(self.model, self.train_loader, self.optimizer, self.criterion, self.device)\n",
        "           print(\"Local training loss:\", local_train_loss)\n",
        "        return self.get_parameters(config), len(self.train_loader.dataset), {}\n",
        "\n",
        "    def evaluate(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        loss = evaluate(self.model, self.test_loader, self.criterion, self.device)\n",
        "        return float(loss), len(self.test_loader.dataset), {}\n",
        "\n",
        "# Client Function or Client App\n",
        "def client_fn(context: Context) -> Client:\n",
        "    idx = int(context.node_config[\"partition-id\"])\n",
        "    csv  = csv_paths[idx] # csv file per user that we have created earlier\n",
        "    client_training_data, client_test_data = load_client_data(csv, batch_size=32) # Creating training data and test data from those per user/clent csv file\n",
        "    model  = RecommenderMLP(num_users, num_movies, CONFIG[\"embed_dim\"], CONFIG[\"hidden_dim\"])\n",
        "    return FLClient(model, client_training_data, client_test_data, device=torch.device(CONFIG[\"device\"])).to_client()"
      ],
      "metadata": {
        "id": "C8B6PA0-8gOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL Server\n"
      ],
      "metadata": {
        "id": "DaDjUBlhV9Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Server Function or Server App which has strategy how to FL will run\n",
        "def server_fn(context: Context) -> ServerAppComponents:\n",
        "    strategy = FedAvg(\n",
        "        fraction_fit=1,\n",
        "        fraction_evaluate=1,\n",
        "        min_fit_clients=CONFIG[\"num_clients\"],\n",
        "        min_evaluate_clients=CONFIG[\"num_clients\"],\n",
        "        min_available_clients=CONFIG[\"num_clients\"],\n",
        "    )\n",
        "    config = ServerConfig(num_rounds=CONFIG[\"num_rounds\"])\n",
        "    return ServerAppComponents(strategy=strategy, config=config)"
      ],
      "metadata": {
        "id": "jCgdF1hoWGbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Runner"
      ],
      "metadata": {
        "id": "C0f4GhUu_79o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_federated():\n",
        "  global csv_paths, num_users, num_movies # we need these in Client Function\n",
        "  df_master = load_master(CONFIG[\"data_path\"])\n",
        "  selected_uids = sample_user_ids(df_master, CONFIG[\"num_clients\"])\n",
        "  folder = f\"clients_{CONFIG['num_clients']}\"\n",
        "  csv_paths = partition_by_user(df_master, folder, selected_uids)\n",
        "\n",
        "  # Set dataset dimensions\n",
        "  num_users = int(df_master[\"UserID\"].max())\n",
        "  num_movies = int(df_master[\"MovieID\"].max())\n",
        "\n",
        "  # Defining client and server app\n",
        "  client_app = ClientApp(client_fn=client_fn)\n",
        "  server_app = ServerApp(server_fn=server_fn)\n",
        "\n",
        "  # Run simulation\n",
        "  start_time = time.perf_counter()\n",
        "  run_simulation(\n",
        "            client_app=client_app,\n",
        "            server_app=server_app,\n",
        "            num_supernodes=CONFIG[\"num_clients\"],\n",
        "            backend_config={\n",
        "                \"client_resources\": {\n",
        "                    \"num_cpus\": 1,\n",
        "                }\n",
        "            },\n",
        "        )\n",
        "  end_time = time.perf_counter()\n",
        "  print(f\"Time taken: {end_time - start_time:0.4f} seconds\")\n",
        "  # for r, mse in history.losses_distributed:\n",
        "  #       print(f\"Round {r:02d}: MSE={mse:.4f}, RMSE={np.sqrt(mse):.4f}\")\n",
        "  #       mse_values.append(mse)\n",
        "  #       rmse_values.append(np.sqrt(mse))\n",
        "\n",
        "  # return history, mse_values, rmse_values"
      ],
      "metadata": {
        "id": "R9fb9aBHhMrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triggering Federated Learning"
      ],
      "metadata": {
        "id": "jDw9nfRjAKSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"num_clients\": 100,\n",
        "    \"num_rounds\": 10,\n",
        "    \"num_epochs\": 3,\n",
        "    \"data_path\": \"preprocessed_dataset.csv\",\n",
        "    \"device\": \"cpu\",\n",
        "    \"embed_dim\": 32,\n",
        "    \"hidden_dim\": 64,\n",
        "}\n",
        "\n",
        "run_federated()\n",
        "# history, mse_values, rmse_values   = run_federated()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zOvgWz3XaNX",
        "outputId": "6decb090-8701-4f88-ba16-ef9dc16478ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2292d8e7e956>:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(csv_path)\n",
            "DEBUG:flwr:Asyncio event loop already running.\n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=10, no round_timeout\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=10856)\u001b[0m 2025-05-16 18:53:15.313415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=10856)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=10856)\u001b[0m E0000 00:00:1747421595.369562   10856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=10856)\u001b[0m E0000 00:00:1747421595.383326   10856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.653217315673828\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.112255096435547\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.579436302185059\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.119012045628816\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.912876888386254\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.843495961531852\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.991059590899756\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.24217476920476\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.525372262984988\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 17.235469818115234\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.67262077331543\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.12274932861328\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.27953815460205\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.405283110482353\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.575954982212611\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.743899873159464\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.858134436375886\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.066379343421714\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.836366653442383\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.294401168823242\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.760758399963379\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 18.868423207600912\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 17.72933489481608\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.645990498860677\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 17.242884318033855\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.37168820699056\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.557593981424967\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.755310496345896\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.751790343737992\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 13.830376156040879\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.811821641593143\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.657846056181809\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 13.58616174500564\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.974973291964144\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.066916620409167\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 13.21218660715464\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.991048812866211\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.579103469848633\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.174633979797363\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.387433494009622\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.267798726151629\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.223913518393912\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.458816528320312\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.354142570495606\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 8.328339227040608\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.888229565448071\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.474564155900335\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 8.144417808716556\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.44132080078125\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.418328094482423\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 13.44772834777832\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 19.62256622314453\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 19.001049041748047\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 18.388803482055664\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 6.68600138255528\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 5.285036672864641\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 4.057932915006365\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.165124553523652\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.632328647456758\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.180137085588012\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.796880300017609\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 6.053342232758972\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 2.606353340477779\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.328305493891714\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 2.4754747618138313\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.451125707542687\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 22.36414282662528\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 20.95901254926409\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 19.67596615382603\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.330659866333008\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.84101390838623\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.364033699035645\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.204044757169836\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.95572187760297\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.74871509776396\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.357149865892199\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 13.19010861714681\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.098095152113173\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.433185407157255\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 3.5881229634596923\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.7672887922447418\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.716277792647078\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.94092490222003\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.212170523566169\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 18.750919342041016\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 18.121084213256836\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 17.506668090820312\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.89826858404911\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.921532197432084\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 7.04192506905758\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 15.081130981445312\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.1241504351298\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.312788009643555\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.379371091622074\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 8.253542867647548\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 6.239907686402198\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 16.632342144569254\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 12.620628270235928\u001b[32m [repeated 48x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 10.720182025243366\u001b[32m [repeated 96x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 7.1552000859888585\u001b[32m [repeated 61x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 10.489712380074167\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.8214378176110515\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 9.572186946868896\u001b[32m [repeated 51x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 7.606056232263546\u001b[32m [repeated 63x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 6.245263985225132\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 0.9066524335954074\u001b[32m [repeated 75x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 11.047492027282715\u001b[32m [repeated 119x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m  5.046522605850036\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 14.178390502929688\u001b[32m [repeated 72x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 5.228576195360434\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 2.0272179322365003\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 5.892742156982422\u001b[32m [repeated 72x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 5.058626634934369\u001b[32m [repeated 114x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 8.409839630126953\u001b[32m [repeated 57x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m  1.6652945919734676\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 5.3795086104294345\u001b[32m [repeated 124x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.0391317538055433\u001b[32m [repeated 99x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.7267802270253498\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 2.290820002555847\u001b[32m [repeated 103x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 6.149859442029681\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 3.2527729534521335\u001b[32m [repeated 104x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 0.9182697363283442\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m 1.10244104511294\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.9989697544471077\u001b[32m [repeated 107x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 2.6702483221165183\u001b[32m [repeated 99x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 1.6074303308379985\u001b[32m [repeated 119x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m  3.573167779228904\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 3.9681108633677167\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m 0.7851010390690395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 0.7157624626159668\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m  1.9016839265823364\n",
            "\u001b[36m(ClientAppActor pid=10856)\u001b[0m Local training loss: 0.9990793550223634\u001b[32m [repeated 113x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m  3.0105338096618652\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 1.4800258634066341\u001b[32m [repeated 87x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 100 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 0.7277721977233886\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 1.3910060447195303\u001b[32m [repeated 127x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 1.1617736233605278\u001b[32m [repeated 125x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 100 clients (out of 100)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 100 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 10 round(s) in 224.20s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 9.947847074384526\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 7.648478051608073\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 5.834322568867449\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 4.4725045909434105\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 3.517450380472675\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 6: 2.851048617322077\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 7: 2.3912904856258517\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 8: 2.080417061982043\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 9: 1.8696711163302484\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 10: 1.7279042162858214\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=10855)\u001b[0m Local training loss: 1.1007659718932876\u001b[32m [repeated 45x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=10855)\u001b[0m 2025-05-16 18:53:15.312993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=10855)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=10855)\u001b[0m E0000 00:00:1747421595.368175   10855 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=10855)\u001b[0m E0000 00:00:1747421595.383369   10855 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 252.3693 seconds\n"
          ]
        }
      ]
    }
  ]
}